{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSNnWlZYJqo8"
      },
      "source": [
        "# Big Data Analytics - Lab 03"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH2tprfcJmoe"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################################\n",
        "## This was no longer working as of 9/9/2025 | MCM ##\n",
        "## I also tested runtime 2025.07             | MCM ##\n",
        "#####################################################"
      ],
      "metadata": {
        "id": "4yarVC0w_pWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08uvTXUwmurH"
      },
      "outputs": [],
      "source": [
        "# Do not change or modify this cell\n",
        "# Need to install pyspark\n",
        "# if pyspark is already installed, will print a message indicating requirement already satisfied\n",
        "#! pip install pyspark >& /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrteBOcxnDFB"
      },
      "outputs": [],
      "source": [
        "# Create Spark Session\n",
        "#from pyspark.sql import SparkSession\n",
        "#spark = SparkSession.builder.appName('BDA-Lab-03').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "## These updates added 9/10/2025 | MCM ##\n",
        "#########################################"
      ],
      "metadata": {
        "id": "Ni3UJ6da_4CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change or modify this cell\n",
        "! pip install pyspark==3.5.1 delta-spark findspark"
      ],
      "metadata": {
        "id": "k6flPmeo_1Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Spark Session\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
        "\n",
        "import pyspark\n",
        "from delta import *\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as f\n",
        "from delta.pip_utils import configure_spark_with_delta_pip\n",
        "\n",
        "builder = (\n",
        "    pyspark.sql.SparkSession.builder.appName(\"BDA-Lab-03\")\n",
        "    .config(\n",
        "        \"spark.sql.extensions\"\n",
        "        ,\"io.delta.sql.DeltaSparkSessionExtension\"\n",
        "      )\n",
        "    .config(\n",
        "        \"spark.sql.catalog.spark_catalog\"\n",
        "        ,\"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n",
        "        ,\n",
        "    )\n",
        ")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
      ],
      "metadata": {
        "id": "1eap-1wr_Xts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3FG_8HYnMNX"
      },
      "source": [
        "## Unstructured Data Storage Formats\n",
        "\n",
        "From this week's reading in *Essential PySpark for Scalable Data Analytics*:\n",
        "\n",
        "> Unstructured data is any data that is not represented by a predefined data model and can be either human or machine-generated. For instance, unstructured data could be data stored in plain text documents, PDF documents, sensor data, log files, video files, images, audio files, social media feeds, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwv0LL4Qu_QI"
      },
      "source": [
        "Let's start by importing a directory of images, an unstructured data format. We'll then convert this data to a structured format, a `DataFrame`, and write it to a data sink."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VgHcaPxsHDr"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "if [[ ! -f images.zip ]]; then\n",
        "   # download the data file from s3 and save it the local environment\n",
        "   wget https://syr-bda.s3.us-east-2.amazonaws.com/images.zip - q\n",
        "   unzip images.zip\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etYuMW1SvGax"
      },
      "source": [
        "Note that we connect to this entire directory of files as a single source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7yh_oUDnGiA"
      },
      "outputs": [],
      "source": [
        "images = spark.read.format('image')\\\n",
        ".load('./images/')\n",
        "\n",
        "images.printSchema()\n",
        "\n",
        "image_df = images.select('image.origin',\n",
        "                         'image.height',\n",
        "                         'image.width',\n",
        "                         'image.nChannels',\n",
        "                         'image.mode',\n",
        "                         'image.data')\n",
        "\n",
        "image_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0q2OiSuf2p6"
      },
      "source": [
        "In the above section, we did the following:\n",
        "- Load a set of image files using Spark's `image` format, resulting in a `DataFrame`.\n",
        "- Use `printSchema()` which shows us there is nested column called `image` with `origin`, `height`, `width`, `nChannels`, `mode`, and `data`.\n",
        "- Unnest each of these inner attributes to be top-level column names.\n",
        "- Display the new `DataFrame` `image_df`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISXH_JvfBcll"
      },
      "source": [
        "Next, we'll use `image_df`, which reads unstructured image data and converts it to a `DataFrame`, then perform a moderately complex operation with it. We'll time this operation in order to make comparisons later.\n",
        "\n",
        "Note that we have not covered Spark `DataFrames` yet. We'll go in depth on this topic next week."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TuCj0s1CopR"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "from pyspark.sql.functions import max, lit\n",
        "image_df.withColumn('max_width',\n",
        "                    lit(image_df.agg(max('width')).first()[0]))\\\n",
        "                    .where('width == max_width')\\\n",
        "                    .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtUmZi_znAOs"
      },
      "source": [
        "The `data` column is binary type. which is incompatible with `csv`. Before we write the file into a data lake, we need to convert `data` to a base64 string.\n",
        "\n",
        "We'll use the `withColumn()` and `base64()` functions to convert the `data` column in `image_df` to a base64 string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvxw6hoBwV8Z"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import base64\n",
        "image_df2 = image_df.withColumn('data', base64(image_df.data))\n",
        "image_df2.printSchema()\n",
        "image_df2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuxhHzSWoh1l"
      },
      "source": [
        "Write `image_df2` to a data lake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRd9ctC-wNr-"
      },
      "outputs": [],
      "source": [
        "image_df2.write.option('header', 'true')\\\n",
        ".mode('overwrite')\\\n",
        ".csv('./data-lake/images.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a2okrR0zZcE"
      },
      "source": [
        "Note that `images.csv` is actually a directory containing multiple files. The number of `.csv` files in this directory depends on the number of executors and your particular Spark configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-Z5l06byTzz"
      },
      "outputs": [],
      "source": [
        "! ls ./data-lake/images.csv/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ok8_9H3zfcA"
      },
      "source": [
        "## Structured Data Storage Formats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2syh4-x8Vkx"
      },
      "source": [
        "From this week's reading in *Essential PySpark for Scalable Data Analytics*:\n",
        "\n",
        "> **Apache Parquet** is a binary, compressed, and columnar storage format that was designed to be efficient at data storage as well as query performance. Parquet is a first-class citizen of the Apache Spark framework, and Spark's in-memory storage format, called Tungsten, was designed to take full advantage of the Parquet format. Therefore, you will get the best performance and efficiency out of Spark when your data is stored in Parquet format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nP2sQewPd_1-"
      },
      "outputs": [],
      "source": [
        "image_df.write.parquet('./data-lake/images.parquet', mode = 'overwrite')\n",
        "parquet_df = spark.read.parquet('./data-lake/images.parquet')\n",
        "parquet_df.printSchema()\n",
        "parquet_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9axfqS6DFaKI"
      },
      "source": [
        "For comparison, let's time the filtering operation from before on the parquet data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPlthaNv9YWd"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "from pyspark.sql.functions import max, lit\n",
        "parquet_df.withColumn('max_width',\n",
        "                      lit(parquet_df.agg(max('width')).first()[0]))\\\n",
        "                      .where('width == max_width')\\\n",
        "                      .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6HCdBrnGcfJ"
      },
      "source": [
        "**Question**: With which format did the filtering operation run faster? Why?\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBeApdMJEvcJ"
      },
      "source": [
        "## Data Lakehouses and Delta Lake\n",
        "\n",
        "[Delta Lake](https://docs.delta.io/latest/delta-intro.html) is an open source project that enables building a [Lakehouse architecture](https://www.cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf) with Apache Spark. Delta Lake provides [ACID transactions](https://www.databricks.com/glossary/acid-transactions), scalable metadata handling, and unifies streaming and batch data processing on top of existing data lakes, such as S3 (Amazon), ADLS (Microsoft), GCS (Google), and HDFS (Hadoop)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOYRzgKgpQcI"
      },
      "source": [
        "### Setting Up a Delta Lake Session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztM5d6-XE18d"
      },
      "source": [
        "Before using Delta Lake, we'll need to install a version of PySpark that is compatible with Delta Lake, a new package for Spark called `delta-spark`, and create a new Spark session configured to use Delta Lake."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
        "\n",
        "import pyspark\n",
        "from delta import *\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as f\n",
        "from delta.pip_utils import configure_spark_with_delta_pip\n",
        "\n",
        "builder = (\n",
        "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "    .config(\n",
        "        \"spark.sql.catalog.spark_catalog\",\n",
        "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
        "    )\n",
        ")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
      ],
      "metadata": {
        "id": "vBwLrhc6xDyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12nHlZ7XpUAD"
      },
      "source": [
        "### Writing to a Delta Lake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83TBKBDsbYMZ"
      },
      "source": [
        "We'll start by creating and saving a very simple data set, the sales data from the last lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4mbcD_cZ3J-"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
        "\n",
        "sales_data = [\n",
        "    ['JAN', 'NY', 3.],\n",
        "    ['JAN', 'PA', 1.],\n",
        "    ['JAN', 'NJ', 2.],\n",
        "    ['JAN', 'CT', 4.],\n",
        "    ['FEB', 'PA', 1.],\n",
        "    ['FEB', 'NJ', 1.],\n",
        "    ['FEB', 'NY', 2.],\n",
        "    ['FEB', 'VT', 1.],\n",
        "    ['MAR', 'NJ', 2.],\n",
        "    ['MAR', 'NY', 1.],\n",
        "    ['MAR', 'VT', 2.],\n",
        "    ['MAR', 'PA', 3.]\n",
        "]\n",
        "\n",
        "# define a schema for this data set\n",
        "schema = StructType([\n",
        "    StructField('month', StringType(), True),\n",
        "    StructField('state', StringType(), True),\n",
        "    StructField('sales', FloatType(), True)\n",
        "])\n",
        "\n",
        "sales = spark.createDataFrame(sales_data,\n",
        "                           schema = schema)\n",
        "\n",
        "sales.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UdmmgRCcbOJ"
      },
      "source": [
        "Next, we'll save this data in *delta* format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tVK567TcaO0"
      },
      "outputs": [],
      "source": [
        "sales\\\n",
        "  .write.format('delta')\\\n",
        "  .mode('overwrite')\\\n",
        "  .save('./sales')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6H6_MHLc_k0"
      },
      "source": [
        "If we look at the contents of `/sales/`, we can see this is simply a collection of parquet files and a log file. We will inspect this log file soon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nFylZjSc7i1"
      },
      "outputs": [],
      "source": [
        "! ls ./sales/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP9eT2HVp9My"
      },
      "source": [
        "### Reading from a Delta Lake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eeyXY8edS5j"
      },
      "source": [
        "We can read the sales data as delta format, just like we would a csv file or other format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_TUQ1KPdapB"
      },
      "outputs": [],
      "source": [
        "sales_delta = spark\\\n",
        "  .read.format('delta')\\\n",
        "  .load('./sales/')\n",
        "\n",
        "sales_delta.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVN2Fy9lePVJ"
      },
      "source": [
        "Let's add some data to the table. Specifically, we'll update it wil sales data for the month of April."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySDBKPywmwb7"
      },
      "outputs": [],
      "source": [
        "# make sure to keep the same schema\n",
        "april_sales = spark.createDataFrame(\n",
        " [['APR', 'NY', 2.0],\n",
        "  ['APR', 'PA', 1.5],\n",
        "  ['APR', 'CT', 4.2],\n",
        "  ['APR', 'NJ', 2.4],\n",
        "  ['APR', 'WZ', 7.0]],\n",
        " schema = schema\n",
        ")\n",
        "\n",
        "# make sure to change mode to append!\n",
        "april_sales\\\n",
        "  .write.format('delta')\\\n",
        "  .mode('append')\\\n",
        "  .save('./sales')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy1zpj-noj2o"
      },
      "source": [
        "We can see more parquet files have been added to the delta file store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZWSMR2cogfR"
      },
      "outputs": [],
      "source": [
        "! ls ./sales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4qrMrSTov8D"
      },
      "source": [
        "Let's make sure the sales table contains the new April sales data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzyFGlTao5de"
      },
      "outputs": [],
      "source": [
        "sales_delta.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbjqdzeVpHMB"
      },
      "source": [
        "### Updating Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0MJiFNAqkSH"
      },
      "source": [
        "Imagine a scenario in which you find out that past data was not recorded correctly. Traditional database management systems allow for simple table updates, but the nature of a data lake is such that data are distributed across many, many files and often many nodes. This is where Delta Lake improves over a traditional data lake architecture. We can make a change to the data, retain all previous records, and the metadata layer will keep track of which records belong in the current version of the table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK7Ovfplr-IS"
      },
      "source": [
        "We found out that sales for Pennsylvania in the month of January were understated. Instead of 1.0 it should be 1.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQxC7_6LsOjc"
      },
      "outputs": [],
      "source": [
        "sales_delta_table = DeltaTable.forPath(spark, './sales')\n",
        "\n",
        "sales_delta_table.update(\n",
        "  condition = (f.col('month') == 'JAN') & (f.col('state') == 'PA'),\n",
        "  set = { 'sales': f.lit(1.5) }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SS1F4dAuMWY"
      },
      "source": [
        "Let's ensure the update was made."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zbCq4MouCgy"
      },
      "outputs": [],
      "source": [
        "sales_delta.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryU9-DIb-9zA"
      },
      "source": [
        "**Activity**: Notice that we introduced a typo when we updated the April sales data: there is now a 5th state mislabled \"WZ\" (this is supposed to be Washington). Correct this record so the state abbreviation reads \"WA\" instead. Everthing else in the record should stay the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wdlalwyAtOB"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BOZn_uJAu4T"
      },
      "outputs": [],
      "source": [
        "# ensure the update was made\n",
        "sales_delta.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP-p-_ivuPbr"
      },
      "source": [
        "### Inspecting the Change Log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJJU9M2G6gOn"
      },
      "source": [
        "The `history` for `sales_delta_table` (from the log  file) contains a record of every operation performed on the `sales` table — from the initial write operation, to the updates we performed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5_zAORc5ywc"
      },
      "outputs": [],
      "source": [
        "sales_delta_table\\\n",
        "  .history()\\\n",
        "  .select('version', 'timestamp', 'operation', 'operationParameters')\\\n",
        "  .show(truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls ./sales"
      ],
      "metadata": {
        "id": "9jgEr4K1QVwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVuvMzx57Iuy"
      },
      "source": [
        "### Time Travel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7azUY_d7ayD"
      },
      "source": [
        "Sometimes we need to view or analyze a previous version of the Delta table. For example, we want to use the original version of the table, before we made any updates. We can do this by querying a specific version of the table. Note that `0` is the original version of the table, but we could do this for any version, based on the contents of the change log."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03rb-B-yCMCh"
      },
      "outputs": [],
      "source": [
        "spark\\\n",
        "  .read.format('delta')\\\n",
        "  .option('versionAsOf', 0)\\\n",
        "  .load('./sales')\\\n",
        "  .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "887HKbgpC9Aq"
      },
      "source": [
        "**Activity**: Instead of a specific version, we could *time travel* using a point in time as a condition. Inspect the change log and update the following code block so that the query will display the table in its original state (matching above), now using a date-time condition instead of a version condition.\n",
        "\n",
        "Important notes from the Delta Lake [documentation](https://docs.delta.io/latest/delta-batch.html#-deltatimetravel):\n",
        "\n",
        "> For timestamp_string, only date or timestamp strings are accepted. For example, \"2019-01-01\" and \"2019-01-01T00:00:00.000Z\"\n",
        "\n",
        ">The timestamp of each version N depends on the timestamp of the log file corresponding to the version N in Delta table log. Hence, time travel by timestamp can break if you copy the entire Delta table directory to a new location. Time travel by version will be unaffected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfn0Ls6XEUeS"
      },
      "outputs": [],
      "source": [
        "timestamp_string = ''\n",
        "\n",
        "spark\\\n",
        "  .read.format('delta')\\\n",
        "  .option('timestampAsOf', timestamp_string)\\\n",
        "  .load('./sales')\\\n",
        "  .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JIAk687F7Ho"
      },
      "source": [
        "Time travelling, as we just did, does not change the Delta table itself — it only shows us a previous version of the table. If we want to fully *roll back* the Delta table, we use the RESTORE operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDWVn2qBHKYQ"
      },
      "outputs": [],
      "source": [
        "sales_delta_table.restoreToVersion(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_HuZuJNHRBp"
      },
      "outputs": [],
      "source": [
        "spark\\\n",
        "  .read.format('delta')\\\n",
        "  .load('./sales')\\\n",
        "  .show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls ./sales"
      ],
      "metadata": {
        "id": "6p5XaN1HRwgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arCH00tSHnu8"
      },
      "outputs": [],
      "source": [
        "sales_delta_table\\\n",
        "  .history()\\\n",
        "  .select('version', 'timestamp', 'operation', 'operationParameters')\\\n",
        "  .show(truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydi7dpHcHxJE"
      },
      "source": [
        "### Merging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImogbaWpVA__"
      },
      "source": [
        "Now that we have rolled back (reverted) our sales data to its original state, we no longer have any sales data for April. Suppose we obtained data from another source which includes: the April sales data (without the typo this time); the missing sales data for Washington for the previous 3 months; the corrected record for Pennsylvania in January; and some, but not all, of the data we already have.\n",
        "\n",
        "**How can we use this data to update out current Delta table?**\n",
        "\n",
        "According to the Delta Lake [documentation](https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge&language-python):\n",
        "\n",
        "> You can upsert data from a source table, view, or DataFrame into a target Delta table by using the MERGE SQL operation. Delta Lake supports inserts, updates and deletes in MERGE, and it supports extended syntax beyond the SQL standards to facilitate advanced use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S7vmnAEL2tB"
      },
      "source": [
        "**Activity:** Read through the merge example provided in the Delta Lake documentation linked above, and adapt it to merge the following data update with the current version of our Delta table. Since we have a mix of new data and old data (but not all of the old data), this is more complex than a simple append. Specifically, we want to insert records we don't currently have, make changes to current records if there is a change (the bad entry from Pennsylvania in January), and leave all other records alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z6Zbrd1Lrkf"
      },
      "outputs": [],
      "source": [
        "sales_updates = spark.createDataFrame(\n",
        " [['APR', 'NY', 2.0],\n",
        "  ['APR', 'PA', 1.5],\n",
        "  ['APR', 'CT', 4.2],\n",
        "  ['APR', 'NJ', 2.4],\n",
        "  ['APR', 'WA', 7.0],\n",
        "  ['JAN', 'WA', 7.0],\n",
        "  ['FEB', 'WA', 7.0],\n",
        "  ['MAR', 'WA', 7.0],\n",
        "  ['JAN', 'PA', 1.5],\n",
        "  ['JAN', 'CT', 4.0],\n",
        "  ['FEB', 'PA', 1.0],\n",
        "  ['FEB', 'NJ', 1.0],\n",
        "  ['FEB', 'VT', 1.0]],\n",
        " schema = schema\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLPUV04eOpb1"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4wQdXTHRyZO"
      },
      "outputs": [],
      "source": [
        "sales_delta\\\n",
        "  .orderBy('month', 'state')\\\n",
        "  .show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sales_delta_table\\\n",
        "  .history()\\\n",
        "  .select('version', 'timestamp', 'operation', 'operationParameters')\\\n",
        "  .show(truncate = False)"
      ],
      "metadata": {
        "id": "I_ObqJq3HcI4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}