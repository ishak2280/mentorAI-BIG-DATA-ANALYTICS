{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Big Data Analytics - Lab 02\n",
        "\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "2d16c646-c9e7-4707-8b0f-3dea3ede872c",
          "inputWidgets": {},
          "title": ""
        },
        "id": "9nCGf_k7HiJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up our Spark environment\n",
        "The next cell installs PySpark in the Google Colab environment. Spark is written in Scala and runs in a Java Virtual Machine. PySpark is a Python interface to a Spark backend virtual machine (VM). There are Java, Python, R, Scala and SQL frontend interfaces to Spark. Essentially, PySpark sends the Python Spark commands to the Spark VM for evaluation, then the results are returned to the PySpark frontend."
      ],
      "metadata": {
        "id": "MPsPrKqWHoS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change or modify this cell\n",
        "# Need to install pyspark\n",
        "# if pyspark is already installed, will print a message indicating requirement already satisfied\n",
        "! pip install pyspark >& /dev/null"
      ],
      "metadata": {
        "id": "G_b7BjwDHmQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Spark Session and Spark Context\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('BDA-Lab-02').getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "zHNncXayH5r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to RDDs\n",
        "\n",
        "Resilient Distributed Datasets (RDDs) are the core abstraction of Apache Spark. They are immutable data structures that can reside in the memory of multiple machines."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "f0f4d6c7-de8f-4d18-8c08-6a23287b1cf3",
          "inputWidgets": {},
          "title": ""
        },
        "id": "ty05dSP1HiJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create an RDD from Python objects using the `parallelize` function from PySpark."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "bf814d49-c60d-453b-84f2-c52e56610091",
          "inputWidgets": {},
          "title": ""
        },
        "id": "S5fS-uN3HiJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(range(20))"
      ],
      "metadata": {
        "id": "vs0LS_LVv3Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize(range(20))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "21a550b7-8971-4e46-abc4-a1874e958bfc",
          "inputWidgets": {},
          "title": ""
        },
        "id": "H2C2fskjHiJo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the RDD"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "f947c103-efa2-4235-a482-6f758ea43b0c",
          "inputWidgets": {},
          "title": ""
        },
        "id": "OChK-dEhHiJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(rdd)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "c77ef271-61b8-4b1d-acc0-a09efca0f379",
          "inputWidgets": {},
          "title": ""
        },
        "id": "T-iw7ie9HiJp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the RDD type"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "ae007bff-cdf0-4138-ba79-6361a9cb8ea7",
          "inputWidgets": {},
          "title": ""
        },
        "id": "4_Bgk072HiJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(rdd)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "fce58163-87c9-47bc-8c85-5327c88445c7",
          "inputWidgets": {},
          "title": ""
        },
        "id": "adOya7LMHiJq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the first element of the RDD"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "23fb3382-e886-49ab-8bbf-85da49dab73a",
          "inputWidgets": {},
          "title": ""
        },
        "id": "YmiVWSw8HiJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.first()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "13243f74-5523-4877-9a04-f14872d194ea",
          "inputWidgets": {},
          "title": ""
        },
        "id": "bJSUW0kSHiJq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Python list containing the first 2 elements of the RDD. The `take` method is a heavyweight operation because data has to be transferred from HDFS into the Python interpreter's memory space. If you only take 2 then it's not a big deal but the more you take the heavier the operation becomes."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "5aef8dfd-9e96-44f2-a057-f5fe93fe7b0f",
          "inputWidgets": {},
          "title": ""
        },
        "id": "jG9e8iHUHiJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.take(9)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "facde24d-a3b8-41be-b98e-2bf172b3903b",
          "inputWidgets": {},
          "title": ""
        },
        "id": "hU8NPnWBHiJr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Python list containing all elements in the RDD.  Note that this is a *very* expensive operation since all of the data in the Spark Java VM memory space has to be collected and transferred into the Python interpreter's memory space."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "87172b83-a076-4903-ac15-8c0d983cc21f",
          "inputWidgets": {},
          "title": ""
        },
        "id": "1RrvZlh3HiJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.collect()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "4d23f021-5b33-4a51-87be-485a0551ce13",
          "inputWidgets": {},
          "title": ""
        },
        "id": "iNkRamtCHiJr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can apply functions to each element.  Let's define such a function."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "d4a34fe5-ad80-4d51-bdf9-ad4ba39bfd3c",
          "inputWidgets": {},
          "title": ""
        },
        "id": "_VG2QUceHiJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def less_than_10(x):\n",
        "    if x < 10:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "b0241e05-1716-499d-8a1f-a63688ffc4df",
          "inputWidgets": {},
          "title": ""
        },
        "id": "w2rHsDHnHiJr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by using the `filter` method of `rdd`."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "af77a78e-7099-47a8-b6da-27fb2ac5ce23",
          "inputWidgets": {},
          "title": ""
        },
        "id": "jEvzW_f_HiJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.filter(less_than_10) # note that RDDs are \"lazy\" â€” they will not execute until we call an \"action\""
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "5f416b99-1075-401d-ac58-1c6fe2255ff1",
          "inputWidgets": {},
          "title": ""
        },
        "id": "dom4HIi5HiJr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.filter(less_than_10).collect() # collect() is such an action"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "12499b7f-e838-4a57-83d2-353dd695dabb",
          "inputWidgets": {},
          "title": ""
        },
        "id": "OxvTiruZHiJr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.filter(less_than_10).count() # so is count()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "9c70315d-d497-442b-b2fc-b68884b4ab0a",
          "inputWidgets": {},
          "title": ""
        },
        "id": "lrVQruklHiJr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember when we said RDDs are immutable? If we convert the rdd to a Python list, all original values are unchanged."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "240afedc-005c-4913-98b6-149d93141d4b",
          "inputWidgets": {},
          "title": ""
        },
        "id": "_M-zifs7HiJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.collect()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "c0ed3133-de0b-4399-a836-ad53ed32d466",
          "inputWidgets": {},
          "title": ""
        },
        "id": "EZkGWsrBHiJs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use a `lambda` function for filtering."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "7d1a8d9e-13d3-4c55-9059-45469bf4b18b",
          "inputWidgets": {},
          "title": ""
        },
        "id": "TP5za_EeHiJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.filter(lambda x: x < 10).collect() # remember to call collect in order to see the results"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "cf28e32f-deea-4bc0-b584-8de3a72d80f9",
          "inputWidgets": {},
          "title": ""
        },
        "id": "61T7YOsyHiJs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can apply functions to elements of an RDD using `map` or `flatMap`. Let's start by defining a function named `square` to apply to each element of `rdd`."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "b99925b1-9277-4973-a5c6-331380d6882f",
          "inputWidgets": {},
          "title": ""
        },
        "id": "EczgBK9vHiJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def square(x):\n",
        "    return x**2"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "ea6ed416-ce15-4b8d-b97f-978d35cbbd90",
          "inputWidgets": {},
          "title": ""
        },
        "id": "jhJhduH1HiJs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the square function to each element of `rdd` using the `map` function."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "f39561ee-2b0c-41f1-8527-47232d0ccf4d",
          "inputWidgets": {},
          "title": ""
        },
        "id": "5gnU2bMqHiJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.map(square).collect()[:7] # we'll look at only the first 7 elements"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "a071a724-96f7-47d5-8543-941842e056a5",
          "inputWidgets": {},
          "title": ""
        },
        "id": "3ZAJ5mHpHiJs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use a `lambda` function with `map`."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "38f1b19b-754e-41f0-b737-e51a01784e64",
          "inputWidgets": {},
          "title": ""
        },
        "id": "Pdw9nuylHiJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.map(lambda x: x**2).collect()[:7]"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "b1e3f3f1-2d1d-4d7a-a2ca-5771b6a5e86d",
          "inputWidgets": {},
          "title": ""
        },
        "id": "6gOvy_a0HiJs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activity\n",
        "Let's re-use the function you created in lab 1 that checks if a number is prime. Apply that function to our `rdd` to return a list of prime numbers only."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "f1469ea1-11da-44e8-9fb7-4774edb3e618",
          "inputWidgets": {},
          "title": ""
        },
        "id": "6NQks59rHiJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "d5558f53-311d-4e81-8f8a-b55066eacac8",
          "inputWidgets": {},
          "title": ""
        },
        "id": "ZBpKltqwHiJs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MapReduce\n",
        "\n",
        "In the next section, we will walk through some MapReduce excercises in order to develop an understanding for how MapReduce works."
      ],
      "metadata": {
        "id": "Z5tmS7Q_xPIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classic MapReduce paradigm can be accomplished by using `map`, `flatMap`, and `reduceByKey`."
      ],
      "metadata": {
        "id": "EfID42poxTKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing **total** orders per month"
      ],
      "metadata": {
        "id": "ccK6Uo1jyF0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following RDD contains month, state, and number of orders per month."
      ],
      "metadata": {
        "id": "gyJfSs1kHJMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a python list\n",
        "sales = [\n",
        "['JAN', 'NY', 3.],\n",
        "['JAN', 'PA', 1.],\n",
        "['JAN', 'NJ', 2.],\n",
        "['JAN', 'CT', 4.],\n",
        "['FEB', 'PA', 1.],\n",
        "['FEB', 'NJ', 1.],\n",
        "['FEB', 'NY', 2.],\n",
        "['FEB', 'VT', 1.],\n",
        "['MAR', 'NJ', 2.],\n",
        "['MAR', 'NY', 1.],\n",
        "['MAR', 'VT', 2.],\n",
        "['MAR', 'PA', 3.]]\n",
        "\n",
        "# use the parellize method to convert this list to an RDD\n",
        "sales_rdd = sc.parallelize(sales)"
      ],
      "metadata": {
        "id": "lWDFh4OvHF78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the map function to apply to each element of the RDD."
      ],
      "metadata": {
        "id": "1K8Txz7-HNmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_func(row):\n",
        "    return [row[0], row[2]]"
      ],
      "metadata": {
        "id": "r1RZtC5oyJDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** What does this function do?"
      ],
      "metadata": {
        "id": "pYiMpnFeyWpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply `map_func` to each element of the RDD."
      ],
      "metadata": {
        "id": "MWgNTPQIysWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"raw data:\", sales_rdd.collect())\n",
        "print(\"mapped data:\", sales_rdd.map(map_func).collect())"
      ],
      "metadata": {
        "id": "9fS8Cg6Hy2b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, reduce to count the number of orders per month."
      ],
      "metadata": {
        "id": "kVw8gvZOzGYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_func(value1, value2):\n",
        "    return value1 + value2"
      ],
      "metadata": {
        "id": "AszCfcRZzNON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put it all together."
      ],
      "metadata": {
        "id": "F6WHEK3UzPB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_rdd.map(map_func).reduceByKey(reduce_func).collect()"
      ],
      "metadata": {
        "id": "c1uFLbeezUW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing **average** orders per month\n",
        "\n",
        "The cell below defines a function which will be called in the map function. The `avg_map_func` takes a row from the rdd defined above, and returns the value in the first col, and a tuple containing the the value in the 3rd col followd by a 1.  The 1 will be used in the reducer to count the number of items for the key where the key is the month."
      ],
      "metadata": {
        "id": "QBnzla6X0vJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MONTH_INDEX = 0\n",
        "ORDER_INDEX = 2\n",
        "\n",
        "def avg_map_func(row):\n",
        "    return (row[MONTH_INDEX], (row[ORDER_INDEX], 1))"
      ],
      "metadata": {
        "id": "juyfLh_y1GSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `avg_reduce_func` takes value 1 and value 2 as inputs. Value 1 and value 2 are expected to be the tuples defined in the output from `avg_map_func` above. The goal of the function is to add up the floats and the 1's in the tuples. We are essentially summing up the floats and the 1's associated with each unique key. Note that the key is not one of the args, the `reduceByKey` function below will strip the keys out of the data returned by the map function."
      ],
      "metadata": {
        "id": "evMXdHaU1J8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "COUNT_INDEX = 1\n",
        "NUM_ORDER_INDEX = 0\n",
        "\n",
        "def avg_reduce_func(value1, value2):\n",
        "    # (current sum of orders + new num orders), (current number of keys + new num keys)\n",
        "    return ((value1[NUM_ORDER_INDEX] + value2[NUM_ORDER_INDEX], value1[COUNT_INDEX] + value2[COUNT_INDEX]))"
      ],
      "metadata": {
        "id": "4Dh0kt7C1U3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test out `avg_map_func`."
      ],
      "metadata": {
        "id": "r3ffCqPM1dpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_rdd.map(avg_map_func).collect()"
      ],
      "metadata": {
        "id": "Y5ihcNQ01ZdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we test the `map` and `reduceByKey` functions. The `map` function returns the month (used as the key for the `reduceByKey` function), and a tuple containing the 3rd col floating point value followed by a 1."
      ],
      "metadata": {
        "id": "LzZHMqQP1nsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_rdd.map(avg_map_func).reduceByKey(avg_reduce_func).collect()"
      ],
      "metadata": {
        "id": "j4POuk2r1xzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we present 2 different ways to compute the final average using `map` and `mapValues` functions to divide the sum of the floats by the sum of the 1's.  The `mapValues` function excludes the keys so there is no need for double indexing. The sum of the 1's is the number of rows per key so the result is the average."
      ],
      "metadata": {
        "id": "tQej-9Vo2AzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOTAL_INDEX = 0\n",
        "print(\"Using mapValues:\",\n",
        "      sales_rdd.map(avg_map_func)\\\n",
        "      .reduceByKey(avg_reduce_func)\\\n",
        "      .mapValues(lambda x: x[TOTAL_INDEX]/x[COUNT_INDEX])\\\n",
        "      .collect())\n",
        "\n",
        "KEY_INDEX = 0\n",
        "VALUE_INDEX = 1\n",
        "TOTAL_ORDER_INDEX = 0\n",
        "COUNT_INDEX = 1\n",
        "print(\"Using map:\",\n",
        "      sales_rdd.map(avg_map_func)\\\n",
        "      .reduceByKey(avg_reduce_func)\\\n",
        "      .map(lambda x: (x[KEY_INDEX], x[VALUE_INDEX][TOTAL_ORDER_INDEX]/x[VALUE_INDEX][COUNT_INDEX]))\\\n",
        "      .collect())"
      ],
      "metadata": {
        "id": "-uCvkZtP2INJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counting words in Shakespeare's collected works using **MapReduce**\n",
        "\n",
        "We start by downloading data from a remote source. The `shakespeare.txt` file contains the complete works of William Shakespeare, obtained from Project Gutenburg (https://www.gutenberg.org/ebooks/100)"
      ],
      "metadata": {
        "id": "QiPk2YRF28hE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "if [[ ! -f shakespeare.txt ]]; then\n",
        "   # download the data file from s3 and save it the local environment\n",
        "   wget https://syr-bda.s3.us-east-2.amazonaws.com/shakespeare.txt -q\n",
        "fi"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "3da378af-8e73-4a3b-af44-2fdb805af341",
          "inputWidgets": {},
          "title": ""
        },
        "id": "YFmMkMZ2HiJ1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an RDD from the downloaded text file, then print its unique identifier."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "d00b366a-ba33-40a1-b74e-6e2838296b3e",
          "inputWidgets": {},
          "title": ""
        },
        "id": "cINe4U5iHiJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_rdd = sc.textFile('shakespeare.txt')\n",
        "shakespeare_rdd.id()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "57f5aba4-3215-43a4-9df9-a1b671319e8c",
          "inputWidgets": {},
          "title": ""
        },
        "id": "y48HnuIwHiJ2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_rdd.first()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {
            "rowLimit": 10000,
            "byteLimit": 2048000
          },
          "nuid": "e69adfa0-b18f-4f8f-b29e-acbbf163cc27",
          "inputWidgets": {},
          "title": ""
        },
        "id": "duG9EqdzHiJ2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the call to `first` actually returns a Python string."
      ],
      "metadata": {
        "id": "RNHcTbWBu-S2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(shakespeare_rdd.first())"
      ],
      "metadata": {
        "id": "rw7UHXTsu90I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the first 10 elements of the RDD to a python list."
      ],
      "metadata": {
        "id": "mkPRvqdhvGA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_rdd.take(10)"
      ],
      "metadata": {
        "id": "Km6TI49rvGjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check how many times the word `love` appears"
      ],
      "metadata": {
        "id": "eUKpOk7QvPzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_love(line):\n",
        "    return line.lower().split().count('love')"
      ],
      "metadata": {
        "id": "9l56PnZPvSZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_rdd.map(count_love).take(10)"
      ],
      "metadata": {
        "id": "uvDt2_fuvX5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_rdd.map(count_love).sum()"
      ],
      "metadata": {
        "id": "AFl3NhIOvdKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def has_love(line):\n",
        "    # should return True if line has word `love`, and False otherwise\n",
        "    return \"love\" in line.lower()"
      ],
      "metadata": {
        "id": "7nhR1YFqvjiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_rdd.filter(has_love).take(3)"
      ],
      "metadata": {
        "id": "RUYrfs7Tvmr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activity\n",
        "use a `lambda` function to achieve the same result as above."
      ],
      "metadata": {
        "id": "RylOoYvKwSCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "L1zv24vSJqIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's count every word in `shakespeare_rdd`."
      ],
      "metadata": {
        "id": "h37cJnlWJL1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define utility functions to be used by `flatMap` and `reduceByKey`"
      ],
      "metadata": {
        "id": "cCB8RZrO4_jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(corpus):\n",
        "    return [(word.lower(), 1) for word in corpus.split()]\n",
        "\n",
        "def sum_words(first, second):\n",
        "    return first + second"
      ],
      "metadata": {
        "id": "dQFK_c4b5GbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break up the `flatMap` and `reduceByKey` operations. The `flatMap` operation takes a single element (in this case a list of words), and returns 0 or more output items."
      ],
      "metadata": {
        "id": "-bap6JE25TgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_rdd.flatMap(count_words).take(25)"
      ],
      "metadata": {
        "id": "9KPXjVLW5Zkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For comparison purposes only, here is what happens if we use `map` instead of `flatMap`. Notice how `map` returns a list of lists while `flatMap` returns a single list."
      ],
      "metadata": {
        "id": "84cv0b9Q5kfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Why would this structure be problematic?"
      ],
      "metadata": {
        "id": "xGcYNSQxhU-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_rdd.map(count_words).take(5)"
      ],
      "metadata": {
        "id": "QbpeOcg_5pzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, when we add the `reduceByKey` function onto the `flatMap` function, the `reduceByKey` function groups common words by key, and adds up all the ones associated with each word/key."
      ],
      "metadata": {
        "id": "dh5C9QQW59vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_rdd.flatMap(count_words).reduceByKey(sum_words).take(10)"
      ],
      "metadata": {
        "id": "9XlS-PkF6GFx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "BDA-Lab-02",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 4,
        "mostRecentlyExecutedCommandWithImplicitDF": {
          "commandId": -1,
          "dataframes": [
            "_sqldf"
          ]
        }
      },
      "language": "python",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}